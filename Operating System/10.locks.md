# Concurrency : Locks

## Locks

### Why we need locks

2개의 스레드가 동시에 아래 코드를 실행할 때를 보자.

```c
extern int g;
void inc()
{
    g++;
}
```

이 코드를 어셈블리어로 바꾸면, 아래와 같다.

```
movl 0x1000, %eax
addl $1, %eax
movl %eax, 0x1000
```

T1이 1을 더한 결과를 저장하기 전에 컨텍스트 스위칭이 발생하면, T2 가 1을 더하고 저장해 버린다.

<img width="439" alt="image" src="https://github.com/ddoddii/OS-CA-Study/assets/95014836/930bdb8c-ff2c-44e8-95cf-026c82b4fcc8">

#### Sharing Resources

- 로컬 변수는 스레드 간 공유되지 않는다.
  - 스레드 마다 스택 공간이 있다.
  - 다른 스레드의 스택으로 로컬 변수의 포인터를 공유/패스/저장하지 않는다.
- 전역 변수는 스레드 사이에 공유된다. 
  - 스태틱 데이터 세그먼트(static data segment)에 저장되고, 어느 스레드나 접근 가능하다.
- 동적 객체(dynamic object)는 스레드 사이에 공유된다.
  - 힙에 저장되고, 포인터를 통해 공유된다. 
- 프로세스끼리는 메모리를 공유할 수 있다.

#### Synchronization Problem

- 동시성(concurrency)는 확정적이지 않은 결과(non-deterministic)를 가져온다. 즉, 실행 순서에 따라 결과가 바뀔 수 있다.
  - 2개 이상의 동시에 실행되는 스레드가 공유자원에 접근할 때, **race condition** 이 발생한다. 
  - 디버그하기 어렵다.
- 따라서 공유 자원에 대한 접근을 제어하기 위해 동기화(synchronization) 메커니즘이 필요하다.
  - 동기화는 동시성을 제어한다.
  - 이 때 스케쥴링은 프로그래머의 제어 영역이 아니다.

#### Concurrency in the Kernel

커널에서도 race condition 이 발생할 수 있다.

<img width="572" alt="image" src="https://github.com/ddoddii/OS-CA-Study/assets/95014836/9479a0e6-61b8-43f2-be0a-5ed23ecdb07a">

#### Critical Section

- 임계구역(critical section)은 공유 자원에 접근하는 코드(변수 또는 자료구조)이다.
- 임계구역에서는 상호 배제(mutual exclusion)이 필요하다.
  - 임계 구역을 원자적으로 실행해야 한다. 즉, 모두 실행되거나 아예 실행되지 말하야 한다.(all-or-nothing)
  - 오직 하나의 스레드만 임계 구역을 실행할 수 있다.
  - 다른 스레드들은 접근하려면 기다려야 한다.
  - 스레드가 임계구역을 떠나면, 다른 스레드가 진입할 수 있다.

### Locks

- 락은 메모리에 있는 객체로, 아래의 두 연산을 통해 상호 배제를 제공한다.
  - `acquire()` : 락이 풀릴 때까지 기다린 다음, 락을 획득한다.
  - `release()` : 락을 풀고, acquire() 에서 기다리고 있는 스레드를 깨운다. 
- 락을 사용할 때,
  - 락은 초기에는 free 상태이다.
  - 임계 구역에 진입하기 전에 `acquire()`를 통해 락을 획득하고, 임계 구역을 나간 후에는 `release()`를 통해 락을 반환한다.
  - `acquire()`시에, 스레드는 스핀(spinlock)하거나 블록(mutex)할 수 있다.
  - 최대 하나의 스레드만 락을 획득할 수 있다.

#### Pthread Mutex

```c
int pthread_mutex_init(pthread_mutex_t *mutex, const pthread_mutexattr_t *mattr);

void pthread_mutex_destory(pthread_mutex_t *mutex);

void pthread_mutex_lock(pthread_mutex_t *mutex); // acquire()

void pthread_mutex_unlock(pthread_mutex_t *mutex); // release()
```

#### Using Locks

```c
int withdraw (account, amount)
{
    acquire(lock); // critical section start
    balance = get_balance(account);
    balance = balance - amount;
    put_balance(account, balance);
    release(lock); // critical section end
    return balance;
}
```

락을 획득하면 무조건 반환해주어야 한다. 반환하지 않으면 데드락 상황이 발생할 수 있다 !

### Using Shared Linked Lists

- 멀티 스레드 어플리케이션에서, **공유 링크드 리스트를 사용하는 상황**을 보자.
- 아래의 상황들이 모두 동시에 일어날 때,
  - 스레드A가 원소a를 삽입함
  - 스레드B가 원소b를 삽입함
  - 스레드C가 원소c를 삽입함



```c
void List_Insert(list_t *L, int key)
{
    node_t *new = malloc(sizeof(node_t));
    assert(new);
    new -> key = key;
    new -> next = L -> head;
    L -> head = new;
}

int List_Lookup(list_t *L, int key)
{
    node_t *tmp = L -> head;
    while (tmp) {
        if (tmp -> key == key)
            return 1;
        tmp = tmp -> next;
    }
    return 0;
}

typedef struct __node_t {
    int key;
    struct __node_t *next;
} node_t;

typedef struct __list_t {
    node_t *head;
} list_t;

void List_Init(list_t *L)
{
    L -> head = NULL;
}
```

#### Linked-List Race

여기서 **Linked-List Race** 가 발생할 수 있다. 

<img width="466" alt="image" src="https://github.com/ddoddii/OS-CA-Study/assets/95014836/61898412-f925-4447-b27d-63384eafe117">

<img width="496" alt="image" src="https://github.com/ddoddii/OS-CA-Study/assets/95014836/5835a242-185a-417a-b72e-3dbe034939b3">

여기서 T2의 노드는 접근 불가능한 노드가 된다.

### Locking Linked Lists

링크드 리스트에 락을 도입해보자. 아래와 리스트 당 한개의 락을 구현할 수 있다. 

```c
void List_Insert(list_t *L, int key)
{
    node_t *new = malloc(sizeof(node_t));
    assert(new);
    new -> key = key;
    new -> next = L -> head;
    L -> head = new;
}

int List_Lookup(list_t *L, int key)
{
    node_t *tmp = L -> head;
    while (tmp) {
        if (tmp -> key == key)
            return 1;
        tmp = tmp -> next;
    }
    return 0;
}

typedef struct __node_t {
    int key;
    struct __node_t *next;
} node_t;

typedef struct __list_t {
    node_t *head;
    pthread_mutex_t lock; // lock
} list_t;

void List_Init(list_t *L)
{
    L -> head = NULL;
    pthread_mutex_init(&L -> lock, NULL); // mutex init
}
```

#### Locking Linked Lists : Approach #1

첫번째 방법은 삽입, 검색 할 때 전부 임계 영역에 넣는다.


```c
void List_Insert(list_t *L, int key)
{   
    pthread_mutex_lock(&L -> lock); // critical section start 
    node_t *new = malloc(sizeof(node_t));
    assert(new);
    new -> key = key;
    new -> next = L -> head;
    L -> head = new;
    pthread_mutex_unlock(&L -> lock); // critical section end
}

int List_Lookup(list_t *L, int key)
{
    pthread_mutex_lock(&L -> lock); // critical section start 
    node_t *tmp = L -> head;
    while (tmp) {
        if (tmp -> key == key)
            return 1;
        tmp = tmp -> next;
    }
    pthread_mutex_unlock(&L -> lock); // critical section end
    return 0;
}
```

그렇다면 임계 구역을 더 작게 만들수는 없을까?

#### Locking Linked Lists : Approach #2

임계 구역을 최대한 작게 만드는 방법이다.

```c
void List_Insert(list_t *L, int key)
{   
    node_t *new = malloc(sizeof(node_t));
    assert(new);
    new -> key = key;
    pthread_mutex_lock(&L -> lock); // critical section start 
    new -> next = L -> head;
    L -> head = new;
    pthread_mutex_unlock(&L -> lock); // critical section end
}

int List_Lookup(list_t *L, int key)
{
    pthread_mutex_lock(&L -> lock); // critical section start 
    node_t *tmp = L -> head;
    while (tmp) {
        if (tmp -> key == key)
            return 1;
        tmp = tmp -> next;
    }
    pthread_mutex_unlock(&L -> lock); // critical section end
    return 0;
}
```

#### Locking Linked Lists : Approach #3

그런데 `Lookup()` 에서 락을 걸 필요가 있을까? 메모리의 값을 읽기만 하는 것은 race condition 이 없다. 

### Requirement for Locks

락은 아래 3가지를 만족시켜야 한다.

1. Correctness
   - Mutual exclusion : 오직 하나의 스레드만 임계 구역 안에 있을 수 있다.
   - Progress(deadlock-free) : 만약 여러 개의 스레드가 임계 구역 안에 들어가고 싶으면, 하나의 스레드가 들어갈 수 있게 해야 한다. 
   - Bounded waiting(starvation-free) : 락을 기다리고자 했던 스레드는 결과적으로 락에 진입할 수 있어야 한다. 

2. Fairness
    - 각 스레드는 락을 획득하는데 공정한 기회가 있어야 한다. 

3. Performance
    - 락이 있을 때의 성능도 고려되어야 한다. 

## Implementing Locks

### Controlling Interrupts

임계 구역 내에서 인터럽트를 비활성화 할 수 있다. 

```c
void acquire(struct lock *l)
{
    cli(); // disable interrupts
}

void release(struct lock *l)
{
    sti(); // enable interrupts
}
```

- 인터럽트 비활성화 하는 것은 컨텍스트 스위칭이 발생할 수 있는 외부 이벤트(e.g. 타이머)가 일어나는 것을 블럭한다.
- 임계 구역 안의 코드는 인터럽트 되지 않을 것이다.
- 그러면 2개의 스레드가 동시에 인터럽트를 비활성화 할 수 있나? -> 가능하다. 

- 장점
  - 간단하다.
  - 싱글 코어 시스템에서 유용한다.
- 단점
  - 커널에서만 구현 가능한다. 왜냐면 인터럽트를 비활성화하는 것은 특권 명령어이기 때문에 유저 수준에서는 구현 불가능하다.
  - 멀티 코어 시스템에서는 충분하지 않다. 왜냐면 각 CPU별로 인터럽트 핸들링을 하니까, 하나의 CPU에서 인터럽트를 비활성화 했어도 의미가 없어진다. 
  - 임계 구역이 길면, 중요한 인터럽트(e.g. timer, disks)가 지연되거나 사라질 수 있다.
  - 현대의 CPU에서 원자적 명령어를 실행하는 것보다 느리다. 

### Multicore 

#### Initial Attempt

스핀락의 초기 구현 형태는 아래와 같다. 

```c
struct lock
{
    int held = 0;
}

void acquire(struct lock *l)
{
    while (l->held); // busy-wait, spins for locks to be released
    l->held = 1;
}

void release(struct lock *l)
{
    l->held = 0;
}
```

위의 코드가 정상적으로 락의 기능을 수행할까? 수행하지 못한다. 왜냐면 2개의 스레드가 동시에 임계 구역으로 진입할 수도 있다. 위에서 락의 조건이었던 상호 배제를 만족하지 못한다. 

문제가 생기는 이유는 held 값을 1로 설정하는 명령어와 0으로 설정해주는 명령어가 분리되어 있기 때문이다.

#### Implemting Locks

- 소프트웨어 알고리즘
  - Dekker's Algorithm
  - Peterson's Algorithm
  - Lamport's Bakery Algorithm for more than two processors
- 하드웨어 원자적 명령어
  - Test-And-Set
  - Compare-And-Swap
  - Load-Linked(LL) and Store-Conditional(SC)
- 인터럽트 제어하기 

#### Test-And-Set

- 원자적 명령어 
  - read-modify-write 명령어가 "원자적"으로 실행된다.
- Test-And-Set 명령어
  - 메모리 위치의 옛날 값을 반환하는 동시에 새로운 값으로 업데이트 한다.

```c
int TestAndSet(int *v, int new)
{
    int old = *v;
    *v = new;
    return old;
}
```

Test-And-Set 을 사용하여 스핀락을 구현할 수 있다.

```c
struct lock {int held = 0;}

void acquire(struct lock *l)
{
    while (TestAndSet(&l -> held, 1));
}

void release(struct lock *l)
{
    l -> held = 0;
}
```

#### Compare-And-Swap

- 옛날 값이 "기대되는 값"과 같을 때에만 메모리 위치를 새로운 값으로 업데이트 한다.
- cmpxchg in x86

```c
int CompareAndSwap(int *v, int expected, int new)
{
    int old = *v;
    if (old == expected)
        *v = new;
    return old;
}

void acquire(struct lock *l)
{
    while(CompareAndSwap(&l->held,0,1));
}
```

#### LL & SC

- MIPS, Alpha, PowerPC, ARM 에서 지원한다.
- Load-Locked(LL)는 메모리에서 값을 가져온다.
- Store-Conditional(SC)는 읽은 메모리 값이 변경이 안되었을 때만 저장에 성공하고, 1을 반환한다. 그렇지 않으면, SC는 메모리를 업데이트하지 않고, 0을 반환한다. 

```c
void acquire(struct lock *l)
{
    while(1) {
        while (LL(&l->held));
        if (SC(&l->held),1) return;
    }
}

void release(struct lock *l)
{
    l->held = 0;
}
```

### Basic Spinlocks ard Unfair

기본 스핀락은 여러 쓰레드가 공유 자원에 접근할 때 동기화를 보장하지만, 그 과정이 공정하지 않을 수 있다. 다음은 그 이유이다:

- 스케줄러 독립성: 스케줄러는 쓰레드가 언제 실행될지를 결정하는데, 이 스케줄러는 락 메커니즘과 독립적으로 동작한다. 락이 해제되었을 때, 다음에 락을 얻는 쓰레드는 순서를 따르지 않고, 단순히 가장 먼저 락을 시도한 쓰레드이다.
- 우선순위 역전: 우선순위가 높은 쓰레드나 자주 스케줄링되는 쓰레드는 락을 독점할 가능성이 높다. 이로 인해 다른 쓰레드가 락을 얻지 못하고 기다리게 되는 상황(기아 상태)이 발생할 수 있다.
- 라이블락: 여러 쓰레드가 동시에 락을 얻으려고 할 때, 지속적으로 회전하면서 진행하지 못하는 상태에 빠질 수 있다


![image](https://github.com/ddoddii/OS-CA-Study/assets/95014836/57ffd6a9-fb43-44a0-8473-6d2ba29d83f5)

### Spinlocks that ensure fairness

#### Fetch-And-Add

- Fetch-And-Add 는 원자적 연산이다. read 를 하고, 그 값에 일정한 값을 더함으로써 다른 스레드가 read + increment 사이에 끼어들 수 없게 한다.
- FAA는 락을 획득하는 과정에서 공정성을 보장한다.
  - 원자적 증가: 쓰레드가 FAA를 사용해 락을 획득하려고 하면, 원자적으로 카운터를 증가시키고, 이 값은 해당 쓰레드의 큐에서의 위치를 나타낸다.
  - 순서 보존: 이렇게 하면 각 쓰레드는 고유한 위치를 받게 되어, 요청한 순서대로 락을 획득할 수 있게 된다.
- xadd in x86 : exchand and add

```c
int FetchAndAdd(int *v, int a)
{
    int old = *v; // 1. 현재 값을 읽어와서 old 변수에 저장
    *v = old + a;  // 2. 값을 a만큼 증가시킴
    return old;  // 3. 이전 값(증가시키기 전의 값)을 반환
}
```

#### Ticket Locks Using Fetch-And-Add

티켓 락은 FAA 메커니즘을 사용하여 공정성을 보장한다. 

1. 티켓 분배: 쓰레드가 acquire 함수를 호출하면, FAA를 사용하여 ticket 카운터를 원자적으로 증가시키고 고유한 티켓 번호를 받는다. 이 번호는 큐에서의 순서를 나타낸다.
2. 순서대로 접근: turn 변수는 현재 락을 획득할 차례인 티켓 번호를 추적한다. 쓰레드는 자신의 티켓 번호가 turn과 일치할 때까지 기다리며, 이렇게 하면 락이 티켓을 받은 순서대로 획득된다.
3. 기아 방지: turn 카운터의 증가가 엄격하게 순차적으로 이루어지기 때문에, 모든 쓰레드는 앞선 티켓 번호가 처리되면 결국 락을 얻을 수 있다. 이렇게 하면 기아 상태를 방지하고 예측 가능한 대기 시간을 제공한다.

```c
struct lock {
    int ticket = 0; // 락을 요청하는 스레드에게 발급되는 티켓 번호
    int turn = 0; // 현재 락을 사용할 차례인 티켓 번호 (락을 해제할 때마다 증가함)
}

void acquire(struct lock *l)
{
    int myturn = FetchAndAdd(&l->ticket, 1); // lock 의 티켓값을 원자적으로 읽고, 1을 증가시킴
    while (l->turn != myturn); // l->turn(현재 락을 사용할 차례인 티켓 번호) 와 myturn 이 같아질 때까지 스레드는 기다림
}

void release (struct lock *l)
{
    l->turn = l->turn + 1; // 다음 티켓 번호를 가진 스레드가 락을 사용할 수 있게 함
}
```

#### CPU Scheduler is Ignorant

CPU 스케줄러는 CPU 시간을 어떻게 분배할지 결정한다. 하지만 스케줄러는 특정 쓰레드가 락을 기다리고 있는지 알지 못한다. 이로 인해 발생할 수 있는 주요 문제는 다음과 같다.

![image](https://github.com/ddoddii/OS-CA-Study/assets/95014836/459f30ec-7c81-4a9a-82e8-7be5943e4433)


1. 비효율적인 CPU 사용
    - 스케쥴러는 락을 얻으려고 하는 쓰레드와 그렇지 않은 쓰레드를 구별하지 못한다.
    - 예를 들어, 락을 기다리는 동안 스케줄러가 B를 실행시키지만, B는 A가 락을 해제할 때까지 기다려야 하므로 실제로 아무런 작업도 하지 않고 CPU 시간을 낭비하게 된다.
2. 스핀락의 비효율성
    - 스핀락은 쓰레드가 락을 얻을 때까지 계속해서 반복적으로 체크하는 방식이다.
    - 이 방식은 락이 빨리 해제되면 효과적이지만, 락이 오래 걸리거나 CPU 시간이 비효율적으로 분배될 때는 오히려 성능 저하를 초래할 수 있다.
3. 불필요한 대기
    - 락을 기다리는 쓰레드(B)가 스케줄링되고 있지만, 실제로는 락을 기다리느라 아무 일도 하지 못하는 경우가 발생할 수 있다.
    - 스케줄러가 락을 기다리는 쓰레드와 락을 소유하고 있는 쓰레드를 구분하지 못하기 때문에, 락이 해제될 때까지 비효율적으로 대기하게 된다.

#### Ticket Locks with yield()

`yield()` 함수를 사용하여 티켓 락을 개선할 수 있다. 기본적인 티켓 락에 yield()를 추가함으로써, 락을 기다리는 동안 CPU 자원을 효율적으로 사용할 수 있다.

`yield()` 함수는 현재 실행 중인 쓰레드가 CPU를 다른 쓰레드에게 양보하고, 자신의 차례가 돌아올 때까지 기다리도록 하는 시스템 호출이다. 이를 통해 락을 기다리는 동안 CPU 자원을 낭비하지 않고 다른 작업을 수행할 수 있게 된다.

```c
struct lock {
    int ticket = 0;
    int turn = 0;
};

void acquire(struct lock *l) {
    int myturn = FetchAndAdd(&l->ticket, 1);
    while (l->turn != myturn)
        yield(); // 내 차례가 아니므로 CPU 양보
}

void release(struct lock *l) {
    l->turn = l->turn + 1;
}
```

![image](https://github.com/ddoddii/OS-CA-Study/assets/95014836/96db18e4-0a64-4e86-9a5d-92e85cb3f894)

### Summary

스핀락은 매우 원시적인 동기화 메커니즘으로, 특정 상황에서는 많은 자원을 낭비할 수 있다. 

1. 스핀락은 자원을 낭비한다.
    - 스핀 상태에서는 진전이 없다:
      - 쓰레드가 락을 획득하기 위해 스핀 상태에 있으면, 락을 보유하고 있는 쓰레드가 실제로 작업을 완료하고 락을 해제할 수 없기 때문에 전체 시스템이 멈추는 상황이 발생할 수 있다.
    - 크리티컬 섹션이 길수록 스핀 시간도 길어진다:
      - 크리티컬 섹션(락이 필요한 코드 부분)이 길어지면, 락을 획득하려는 다른 쓰레드들은 그만큼 오랫동안 기다려야 한다. 이는 CPU 자원의 비효율적인 사용을 초래한다.
    - CPU 사이클의 낭비:
      - 쓰레드가 스핀을 돌면서 계속해서 락의 상태를 체크하면 CPU 사이클이 낭비된다. 이는 다른 유용한 작업에 사용될 수 있는 CPU 자원을 소모한다.
    - 컨텍스트 스위치의 증가:
      - 락을 보유하고 있는 쓰레드가 강제적인 컨텍스트 스위치로 인해 인터럽트될 가능성이 높아집니다. 이는 성능 저하를 초래할 수 있다.

2. 스핀락과 인터럽트 비활성화는 원시적은 동기화 메커니즘이다.
    - 이들은 더 높은 수준의 동기화 구조를 구축하기 위해 사용된다. 